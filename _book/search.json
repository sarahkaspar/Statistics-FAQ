[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "FAQ in statistics",
    "section": "Welcome",
    "text": "Welcome\nWelcome to the Statistics FAQ — your quick-reference guide for common questions in applied statistics and data analysis. Here you’ll find clear explanations, practical examples, and concise answers to many topics that arise in day-to-day research at EMBL.\nFor further support, training, or project consultations, please visit the Data Science Internal Support."
  },
  {
    "objectID": "hypothesis-tests.html",
    "href": "hypothesis-tests.html",
    "title": "Hypothesis tests",
    "section": "",
    "text": "Common hypothesis tests explained."
  },
  {
    "objectID": "Wilcoxon.html#why-non-parametric-tests",
    "href": "Wilcoxon.html#why-non-parametric-tests",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.1 Why non-parametric tests?",
    "text": "1.1 Why non-parametric tests?\nIn the example below, you see data from an experiment where the effect of different conditions on plant growth was investigated.\n\nlibrary(tidyverse)\nmyplants &lt;- PlantGrowth %&gt;% filter(group %in%  c(\"ctrl\", \"trt1\"))\n\nmyplants %&gt;% \n  ggplot(aes(x=group, y = weight))+\n  geom_jitter(width=0.07)\n\n\n\n\nHere, the groups \\(X\\) and \\(Y\\) correspond to “ctrl” and “trt”, and we want to find out whether the weight is different between them. In an example like this, we would typically use a t-test, which calculates the mean and variance of both groups and checks for a difference in means. This comparison of means assumes that the data are somewhat well described by the two group means, meaning that in each group, the data points are roughly centered around their mean, and have no huge outliers. The t-test is a parametric test, because it makes assumptions about the distribution of the data (such as outlined above) and estimates parameters, namely mean and standard deviation, which describe a normal distribution.\nBelow, you see another example, where the t-test is not advised. In the coin::rotarod data set, rats were assigned to two different treatment groups, and of each rat the time it managed to blanace on a rotating cylinder (without falling off) was measured.\n\nlibrary(coin)\n\nLade nötiges Paket: survival\n\nlibrary(ggbeeswarm)\nrotarod %&gt;% \n  ggplot(aes(x=group, y = time))+\n  geom_beeswarm()\n\n\n\n\nThe experiment stopped at 300 s. As you see in the data above, all of the rats in the control group made it to the end without falling, whereas in the treatment group, 5 rats dropped out earlier. This data is not well described by group means and standard deviation, for several reasons:\n\nthe data are censored, i.e. cut off at 300s, and therefore also skewed.\nNo standard deviation can be determined for the control group, because all rats have the same time.\n\nFor this data, a Wilcoxon test is a better choice, because it is non-parametric, i.e. it does not try to fit a distribution to the data. Instead, it transforms the data into ranks, as we will see below."
  },
  {
    "objectID": "Wilcoxon.html#assumptions-null-and-alternative-hypothesis",
    "href": "Wilcoxon.html#assumptions-null-and-alternative-hypothesis",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.2 Assumptions, null and alternative hypothesis",
    "text": "1.2 Assumptions, null and alternative hypothesis\nEven though no specific distribution is assumed when applying the Wilcoxon test, some assumptions are still made (see Wikipedia:\n\nAll the observations from both groups are independent of each other,\nThe responses are at least ordinal (i.e., one can at least say, of any two observations, which is the greater),\nUnder the null hypothesis \\(H_0\\), the distributions of both populations are identical.\n\nIf you don’t know what “independence” means in the statistics context, have a look here.\nTo approach the difference between groups without calculating for example a mean, the null hypothesis is described as follows.\n\\[H_0: P(X &gt; Y) = P(Y &gt; X)\\]\nIn words: If taking any to measurements \\(x_i\\) and \\(x_j\\) from \\(X\\) and \\(Y\\), the chance of \\(x_i\\) being greater than \\(y_i\\) is the same as vice versa.\nThe alternative hypothesis is that this probability is not the same: \\[H_A: P(X &gt; Y) = P(Y &gt; X)\\]\nAn intuitive interpretation is therefore that the test detects differences in median."
  },
  {
    "objectID": "Wilcoxon.html#rank-transformation",
    "href": "Wilcoxon.html#rank-transformation",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.3 Rank transformation",
    "text": "1.3 Rank transformation\nIf we are investigating the probability of \\(Y&gt;X\\), the exact numbers of the data points in \\(X\\) and \\(Y\\) do not matter. For example, if \\(x_i = 2\\) and \\(y_j=2.2\\), then \\(x_i&lt;y_j\\). But if \\(y_j = 200\\) instead, then we still have \\(x_i&lt;y_j\\). It’s only about the order of the data points.\nThe order of data points can be encoded as ranks. For the Wilcoxon test, we rank all the data (ignoring the groups). This means we give rank 1 to the lowest value, rank 2 to the second lowest, and so on. Let’s look at the ranks for a subset of the plants data.\nWe create a subset:\n\nplants_subset &lt;- myplants[c(1:5,11:15),]\nplants_subset\n\n   weight group\n1    4.17  ctrl\n2    5.58  ctrl\n3    5.18  ctrl\n4    6.11  ctrl\n5    4.50  ctrl\n11   4.81  trt1\n12   4.17  trt1\n13   4.41  trt1\n14   3.59  trt1\n15   5.87  trt1\n\n\nNow we assign ranks:\n\nplants_subset &lt;- plants_subset %&gt;% mutate(rank=rank(weight))\nplants_subset %&gt;% arrange(rank)\n\n   weight group rank\n1    3.59  trt1  1.0\n2    4.17  ctrl  2.5\n3    4.17  trt1  2.5\n4    4.41  trt1  4.0\n5    4.50  ctrl  5.0\n6    4.81  trt1  6.0\n7    5.18  ctrl  7.0\n8    5.58  ctrl  8.0\n9    5.87  trt1  9.0\n10   6.11  ctrl 10.0\n\n\nYou can see that the rank columns contains the values from 1 to 10 - or almost. The values 2 and 3 are missing, because there is a tie: The measurement \\(4.17\\) (the second lowest) appears twice. Therefore, both measurements get rank \\(2.5\\)."
  },
  {
    "objectID": "Wilcoxon.html#the-u-statistic",
    "href": "Wilcoxon.html#the-u-statistic",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.4 The U statistic",
    "text": "1.4 The U statistic\nA test statistic is a value that gets extreme (extremely large or extremely small) in case there is a difference between the two group - or, in other words: in case there is evidence for a deviation from the null hypothesis.\nA simple approach to compare the two groups is to sum up the ranks within each group and compare them. We can do this for our example:\n\nplants_subset %&gt;% \n  group_by(group) %&gt;% \n  summarize(rank_sum = sum(rank))\n\n# A tibble: 2 × 2\n  group rank_sum\n  &lt;fct&gt;    &lt;dbl&gt;\n1 ctrl      32.5\n2 trt1      22.5\n\n\nThis tells us that the control group has a lower rank sum, i.e. on average the ranks in the control group are smaller. The next thing we need to find out is how unlikely it is to see a difference in rank sums like this by chance, if in reality \\(X\\) and \\(Y\\) come from the same distribution.\nIntuitively, we can compare the observed rank sums to what we expect in a few extreme cases. For example:\n\nIf the control group had all the lower ranks we would see rank sum \\(R_x = 1+2+3+4+5 = 15\\) for ctrl and \\(R_y = 6+7+8+9+10 = 40\\) for trt1. (And vice versa.)\nIf the ranks were completely evenly distributed among the two groups, we’d see the same rank sum, \\(R_x=R_y=55\\), in both groups.\n\nOur example seems to be somewhat in the middle.\nIn the test, we use the Mann-Whitney U statistic, of which we know the distribution (behavior) under the null hypothesis. It is defined as\n\\[U = min(U_x, U_y)\\] with\n\\[U_x = n_xn_y + \\frac{n_x(n_x + 1)}{2} - R_x \\] and \\[U_y = n_xn_y + \\frac{n_y(n_y + 1)}{2} - R_y \\].\nLet’s dissect this.\n\\(R_x\\) and \\(R_y\\) are the rank sums that we already calculated for our example. In the \\(U_x\\), the rank sum \\(R_x\\) gets subtracted from the term \\[n_xn_y + \\frac{n_x(n_x + 1)}{2}\\] This is the rank sum that we’d expect in case \\(X\\) had all the upper ranks. That expected rank sum depends on \\(n_x\\) and \\(n_y\\), the number of data points in \\(X\\) and \\(Y\\), respectively. We calculated it by hand for the example above, and this calculation can be abstracted to the term you see here.\nSince \\[U_x = \\text{expectation if X has all the upper ranks} - R_x\\], \\(U_x\\) will be\n\n0 if \\(Y\\) in fact has all the upper ranks,\nclose to 0 if \\(X\\) has mostly upper ranks, and\n\n\\(\\gg0\\) if \\(X\\) has intermediate or lower ranks.\n\nThe same logic applies for \\(U_y\\). By taking the minimum of \\(U_x\\) and \\(U_y\\), we will have\n\na \\(U\\) that is low / close to 0 if either \\(X\\) or \\(Y\\) have mostly upper ranks and\nhigh otherwise.\n\nIn summary, a low \\(U\\) represents evidence for \\(X\\) and \\(Y\\) being different, represented as a difference in rank sums."
  },
  {
    "objectID": "Wilcoxon.html#from-u-statistic-to-p-value",
    "href": "Wilcoxon.html#from-u-statistic-to-p-value",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.5 From U statistic to p-value",
    "text": "1.5 From U statistic to p-value\nPer definition, the p-value is the probability of seeing data (i.e. a test statistic) at least as extreme as what as actually observed, under the assumption that the null hypothesis is true.\nIf the probability of our data under the null hypothesis is small (usually: \\(p&lt;0.05\\)), we reject the null hypothesis and report a significant difference between the two groups.\nFor calculating \\(p\\), we need knowledge how the test statistic behaves under the null hypothesis, specifically we need to know its null distribution.\nThere are different ways of getting to this distribution and thus the p-value, which will lead to small differences in the resulting p-value. Two examples are:\n\nCombinatorics: The null hypothesis says that the two groups come from the same distribution and thus the probability of \\(P(X&gt;Y) = P(Y&gt;X)\\). All ranks therefore have the same chance for all the data points. One can thus spell out all combinations of ranks and data points and calculate the corresponding \\(U\\) for getting the null distribution. The p-value is the percentage of possible \\(U\\)s that are smaller or equal to the observed one. (In practice, one doesn’t spell out all possibilities, but combinatorics allows to calculate \\(p\\) directly.)\nFor larger sample sizes (\\(n&gt;20\\)), the distribution of \\(U\\) can be approximated by a normal distribution.\n\nIf you use the R implementation of the wilcox.test, the documentation reads: &gt; By default (if exact is not specified), an exact p-value is computed if the samples contain less than 50 finite values and there are no ties. Otherwise, a normal approximation is used.\nYou are usually fine with sticking to the default. You may see slightly different results from different packages due to small differences in their defaults. Again, in most cases, these don’t impact the conclusion. If they do, it’s time to further investigate which method fits your data best."
  },
  {
    "objectID": "Wilcoxon.html#running-the-test-in-r",
    "href": "Wilcoxon.html#running-the-test-in-r",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.6 Running the test in R",
    "text": "1.6 Running the test in R\nApplying the built-in test to the example subset gives:\n\nwilcox.test(weight ~ group, data = plants_subset)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): kann bei\nBindungen keinen exakten p-Wert Berechnen\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  weight by group\nW = 17.5, p-value = 0.3457\nalternative hypothesis: true location shift is not equal to 0\n\n\nAdvanced: There is another implementation of the Wilcoxon test in the coin package, which allows more flexibility in calculating the test statistic and null distribution, and allows stratifying the test among blocking factors. Example for usage:\n\ncoin::wilcox_test(weight ~ group, data = plants_subset, distribution=\"approximate\")\n\n\n    Approximative Wilcoxon-Mann-Whitney Test\n\ndata:  weight by group (ctrl, trt1)\nZ = 1.0476, p-value = 0.3357\nalternative hypothesis: true mu is not equal to 0\n\n\nWe can also run the test for the whole plantGrowth and rotarod data sets:\n\nwilcox.test(time ~ group, rotarod)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): kann bei\nBindungen keinen exakten p-Wert Berechnen\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  time by group\nW = 102, p-value = 0.01647\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nwilcox.test(weight ~ group, myplants)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): kann bei\nBindungen keinen exakten p-Wert Berechnen\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  weight by group\nW = 67.5, p-value = 0.1986\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "Wilcoxon.html#comparison-to-t-test",
    "href": "Wilcoxon.html#comparison-to-t-test",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.7 Comparison to t-test",
    "text": "1.7 Comparison to t-test\nFor the same data sets the t-test will give the following results:\n\nt.test(weight ~ group, myplants)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by group\nt = 1.1913, df = 16.524, p-value = 0.2504\nalternative hypothesis: true difference in means between group ctrl and group trt1 is not equal to 0\n95 percent confidence interval:\n -0.2875162  1.0295162\nsample estimates:\nmean in group ctrl mean in group trt1 \n             5.032              4.661 \n\n\nThe t-test gives a comparable result for the plantGrowth data set, coming to the same conclusion that the difference in weights is not significant between ctrl and trt1. In most casese where the t-test can be used, it’s also OK to use the Wilcoxon test. One caveat is that in the Wilcoxon test, we drop all the information on the actual values of the data points. This, in combination with making less assumptions on the data, can sometimes make it harder to detect a significant deviation of the data from the our null assumption. This is especially relevant for small data sets. We say that the t-test has a higher power than the Wilcoxon test, meaning that if there is a difference between the groups, the t-test is often better in detecting it.\nThe limitations of the rank-based test become clear when we look at very small data sets. Consider two groups with only 3 data points each. There are only 20 ways to distribute 6 ranks among 2 groups.\n\nchoose(6,3)\n\n[1] 20\n\n\nA two-sided p-value will therefore never be below \\(\\frac{1}{20}\\cdot 2\\). To demonstrate:\n\nx &lt;- 1:3 # x has all the lower ranks\ny &lt;- 4:6 # y has all the upper ranks\nwilcox.test(x,y)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x and y\nW = 0, p-value = 0.1\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "Wilcoxon.html#what-other-methods-to-consider-if-your-data-are-not-normal",
    "href": "Wilcoxon.html#what-other-methods-to-consider-if-your-data-are-not-normal",
    "title": "\n1  The Wilcoxon test\n",
    "section": "\n1.8 What other methods to consider if your data are not normal",
    "text": "1.8 What other methods to consider if your data are not normal\nThe Wilcoxon test is usually used when the data are not normally distributed, so a t-test doesn’t apply. However, there are also other approaches that might apply:\n\nThe data might follow another known distribution around the group means, such as Poisson, or binomial. In this case, you can have a look at Poisson ANOVA or Fisher test.\nThe data might fulfill the normality criterion after a transformation. A common one is the log-transformation."
  }
]